{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#!rm -rf ./logs/\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import filters\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wider cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell confugures all the nessesery information for the results file. File definition is found in the AnalyzeResults notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "initComment = ' Experimenting with dataset variation. Same input and output, unnormalized '\n",
    "datasetSize = 0 # Gets filled out after loading\n",
    "framesSkipped = 5\n",
    "nx = 640\n",
    "ny = 360\n",
    "resolution = (nx, ny)\n",
    "framesBefore = 2\n",
    "framesAfter  = 1\n",
    "baseConvLay  = 32\n",
    "usedLoss     = ' MSE '\n",
    "epochs       = 10\n",
    "\n",
    "chan = 3                                  # Channels per frame, usualy 3 (R, G, B)\n",
    "frames = framesBefore + framesAfter + 1   # Input frames for the model\n",
    "outChan = 1                               # Output channels, usualy 3 (R, G, B)\n",
    "batch_size = 8                            # Batch size for training, lower if you get OOM error\n",
    "skipFramesBefore = 0                      # Skip input frames before mf for loaded dataset\n",
    "skipFramesAfter  = 0                      # Skip input frames after mf for loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../Datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full dataset\n",
    "datasets = ['Skogen', 'Hagen', 'Hytta']\n",
    "NStartOn = [1,         1,       1     ]\n",
    "numFiles = [6,         4,       4     ]\n",
    "#Mini dataset\n",
    "#datasets = ['Skogen']\n",
    "#NStartOn = [1]\n",
    "#numFiles = [6]\n",
    "\n",
    "\n",
    "valDatasets = ['Skogen']\n",
    "valNstartOn = [7]\n",
    "numValFiles = [7]\n",
    "datasetDir = '../../Datasets/DVRUP_Pre_NoNorm/'\n",
    "#datasetDir = '../../Datasets/DVRUP_conv3D_singleCHoutAndInput_float32/'\n",
    "#datasetDir = '../../Datasets/DVRUP_conv3D_singleCHoutput_float32/'\n",
    "def getInputFiles(d, s, n):\n",
    "    ds = []\n",
    "    for i in range(len(d)):\n",
    "        for j in range(s[i], 1 + n[i]):\n",
    "            ds.append(datasetDir + d[i] + str(j) + '.tfrec')\n",
    "    return ds\n",
    "\n",
    "datasetsToLoad = getInputFiles(datasets, NStartOn, numFiles)\n",
    "valSetsToLoad = getInputFiles(valDatasets, valNstartOn, numValFiles)\n",
    "print('training using')\n",
    "for i in datasetsToLoad:\n",
    "    print(i)\n",
    "print('Validating using')\n",
    "for file in valSetsToLoad:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(ex):\n",
    "    features = {\n",
    "    'X': tf.io.FixedLenFeature([], tf.string),\n",
    "    'Y': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    ex = tf.io.parse_single_example(ex, features)\n",
    "    #Decoding the TFRecord\n",
    "    x = tf.io.decode_raw(\n",
    "        ex['X'], out_type=np.float32, little_endian=True, fixed_length=None, name=None\n",
    "    )\n",
    "    y = tf.io.decode_raw(\n",
    "        ex['Y'], out_type=np.float32, little_endian=True, fixed_length=None, name=None\n",
    "    )\n",
    "    x = tf.reshape(x, (360, 640, 4, 1))\n",
    "    y = tf.reshape(y, (360, 640, 1))\n",
    "    return x, y\n",
    "\n",
    "def get_batched_dataset(filenames):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "\n",
    "    dataset = dataset.shuffle(128)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True) \n",
    "    dataset = dataset.prefetch(AUTO) #\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset():\n",
    "    return get_batched_dataset(datasetsToLoad)\n",
    "\n",
    "def get_validation_dataset():\n",
    "    return get_batched_dataset(valSetsToLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSigModel():\n",
    "    kSize = (3,3,3)\n",
    "    act = 'relu'\n",
    "    lact = 'sigmoid'\n",
    "    loss = 'mse'\n",
    "    opt = 'RMSprop' #'adam' #'adadelta'\n",
    "    b_conv = 64\n",
    "    conv_2 = b_conv * 2\n",
    "    conv_3 = b_conv * 4\n",
    "    l_conv = b_conv / 2\n",
    "    reg = tf.keras.regularizers.l1(10e-10)\n",
    "    input_layer = tf.keras.layers.Input(shape = (ny, nx, frames, chan))\n",
    "    #First encoder step\n",
    "    l1 = tf.keras.layers.Conv3D(b_conv, kernel_size = kSize, padding = 'same', activation = act, strides = (2, 2, 2))(input_layer)\n",
    "    #Second encoder step\n",
    "    l2 = tf.keras.layers.Conv3D(conv_2, kernel_size = kSize, padding = 'same', activation = act)(l1)\n",
    "    l3 = tf.keras.layers.Conv3D(conv_2, kernel_size = kSize, padding = 'same', activation = act)(l2)\n",
    "    #Third encoder step\n",
    "    l4 = tf.keras.layers.Conv3D(conv_3, kernel_size = kSize, padding = 'same', activation = act, strides = (2, 2, 2))(l3)\n",
    "    l5 = tf.keras.layers.Conv3D(conv_3, kernel_size = kSize, padding = 'same', activation = act)(l4)\n",
    "    l6 = tf.keras.layers.Conv3D(conv_3, kernel_size = kSize, padding = 'same', activation = act)(l5)\n",
    "    l7 = tf.keras.layers.Conv3D(conv_3, kernel_size = kSize, padding = 'same', activation = act)(l6)\n",
    "    l8 = tf.keras.layers.Conv3D(conv_3, kernel_size = kSize, padding = 'same', activation = act)(l7)\n",
    "    #Decoding from third step\n",
    "    l9 = tf.keras.layers.UpSampling3D((2,2,1))(l8)\n",
    "    l10 = tf.keras.layers.Conv3D(conv_2, kernel_size = kSize, padding = 'same', activation = act)(l9)\n",
    "    l11 = tf.keras.layers.Conv3D(b_conv, kernel_size = kSize, padding = 'same', activation = act)(l10)\n",
    "    l12 = tf.keras.layers.Conv3D(b_conv, kernel_size = kSize, padding = 'same', activation = act)(l11)\n",
    "    #Decoding from third step\n",
    "    l13 = tf.keras.layers.UpSampling3D((2,2,1))(l12)\n",
    "    l14 = tf.keras.layers.Conv3D(l_conv, kernel_size = kSize, padding = 'same', activation = act)(l13)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Conv3D(outChan, kSize, padding = 'same', activation = lact)(l14)\n",
    "\n",
    "    #Autoencoder model\n",
    "    autoencoder = tf.keras.Model(input_layer, output_layer)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer = opt, loss = loss, metrics=['acc'])\n",
    "    return autoencoder\n",
    "\n",
    "def getIntraConvModel():\n",
    "    conv3D_args = {\n",
    "         \"kernel_size\" : (3,3,3),\n",
    "         \"activation\": \"tanh\",\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         \"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "        \n",
    "    conv2D_args = {\n",
    "         \"kernel_size\" : (3,3),\n",
    "         \"activation\": \"relu\",\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         \"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "    \n",
    "    lact = 'sigmoid'\n",
    "    loss = 'MSE'\n",
    "    opt = 'adam' #'adam' #'adadelta'\n",
    "    b_conv = 16\n",
    "    conv_2 = b_conv * 2\n",
    "    conv_3 = b_conv * 4\n",
    "    l_conv = b_conv / 2\n",
    "    input_layer = tf.keras.layers.Input(shape = (ny, nx, frames, chan))\n",
    "    #First encoder step\n",
    "    l1 = tf.keras.layers.Conv3D(b_conv, strides = (2, 2, 1), **conv3D_args)(input_layer)\n",
    "    #Second encoder step\n",
    "    l2 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(l1)\n",
    "    l3 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(l2)\n",
    "    #Third encoder step\n",
    "    l4 = tf.keras.layers.Conv3D(conv_3, strides = (2, 2, 1), **conv3D_args)(l3)\n",
    "    \n",
    "    l5 = tf.keras.layers.Conv3D(conv_3, **conv3D_args)(l4)\n",
    "    l6 = tf.keras.layers.Conv3D(conv_3, strides = (1, 1, 2), **conv3D_args)(l5)\n",
    "    l7 = tf.keras.layers.Conv3D(conv_3, strides = (1, 1, 2), **conv3D_args)(l6)\n",
    "    \n",
    "    lr = tf.keras.layers.Reshape((90, 160, conv_3))(l7)\n",
    "    l8 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(lr)\n",
    "    #Decoding from third step\n",
    "    l9 = tf.keras.layers.UpSampling2D((2,2))(l8)\n",
    "    l10 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l9)\n",
    "    l11 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l10)\n",
    "    l12 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l11)\n",
    "    #Decoding from third step\n",
    "    l13 = tf.keras.layers.UpSampling2D((2,2))(l12)\n",
    "    l14 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l13)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Conv2D(outChan, (3,3), padding = 'same', activation = lact)(l14)\n",
    "\n",
    "    #Autoencoder model\n",
    "    autoencoder = tf.keras.Model(input_layer, output_layer)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer = opt, loss = loss, metrics=['acc'])\n",
    "    return autoencoder\n",
    "\n",
    "def getIntraSingleChModel():\n",
    "    conv3D_args = {\n",
    "         \"kernel_size\" : (3,3,3),\n",
    "         \"activation\": \"tanh\",\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         \"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "        \n",
    "    conv2D_args = {\n",
    "         \"kernel_size\" : (3,3),\n",
    "         \"activation\": \"relu\",\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         \"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "    \n",
    "    lact = 'sigmoid'\n",
    "    loss = 'MSE'\n",
    "    opt = 'adam' #'adam' #'adadelta'\n",
    "    b_conv = 32\n",
    "    conv_2 = b_conv * 2\n",
    "    conv_3 = b_conv * 4\n",
    "    l_conv = b_conv / 2\n",
    "    chan = 1\n",
    "    input_layer = tf.keras.layers.Input(shape = (ny, nx, frames, chan))\n",
    "    #First encoder step\n",
    "    l1 = tf.keras.layers.Conv3D(b_conv, strides = (2, 2, 1), **conv3D_args)(input_layer)\n",
    "    #Second encoder step\n",
    "    l2 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(l1)\n",
    "    l3 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(l2)\n",
    "    #Third encoder step\n",
    "    l4 = tf.keras.layers.Conv3D(conv_3, strides = (2, 2, 1), **conv3D_args)(l3)\n",
    "    \n",
    "    l5 = tf.keras.layers.Conv3D(conv_3, **conv3D_args)(l4)\n",
    "    l6 = tf.keras.layers.Conv3D(conv_3, strides = (1, 1, 2), **conv3D_args)(l5)\n",
    "    l7 = tf.keras.layers.Conv3D(conv_3, strides = (1, 1, 2), **conv3D_args)(l6)\n",
    "    \n",
    "    lr = tf.keras.layers.Reshape((90, 160, conv_3))(l7)\n",
    "    l8 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(lr)\n",
    "    #Decoding from third step\n",
    "    l9 = tf.keras.layers.UpSampling2D((2,2))(l8)\n",
    "    l10 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l9)\n",
    "    l11 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l10)\n",
    "    l12 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l11)\n",
    "    #Decoding from third step\n",
    "    l13 = tf.keras.layers.UpSampling2D((2,2))(l12)\n",
    "    l14 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l13)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Conv2D(outChan, (3,3), padding = 'same', activation = lact)(l14)\n",
    "\n",
    "    #Autoencoder model\n",
    "    autoencoder = tf.keras.Model(input_layer, output_layer)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer = opt, loss = loss, metrics=['acc'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntraSingleChModelMP():\n",
    "    conv3D_args = {\n",
    "         \"kernel_size\" : (3,3,3),\n",
    "         \"activation\": 'tanh',\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         #\"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "        \n",
    "    conv2D_args = {\n",
    "         \"kernel_size\" : (3,3),\n",
    "         \"activation\": 'tanh',\n",
    "         \"kernel_initializer\": \"Orthogonal\",\n",
    "         \"padding\": \"same\",\n",
    "         #\"activity_regularizer\" : tf.keras.regularizers.l1(10e-10),\n",
    "    }\n",
    "    \n",
    "    maxPool_args = {\n",
    "        \"pool_size\" : (2,2,2),\n",
    "        \"strides\" : None,\n",
    "        \"padding\" : \"valid\",\n",
    "    }\n",
    "    \n",
    "    dl_maxPool_args = {\n",
    "        \"pool_size\" : (1,1,2),\n",
    "        \"strides\" : None,\n",
    "        \"padding\" : \"valid\",\n",
    "    }\n",
    "    \n",
    "    lact = 'tanh' # tf.nn.leaky_relu\n",
    "    loss = 'MSE'\n",
    "    opt = 'adadelta' #'adam' #'adadelta'\n",
    "    b_conv = 4\n",
    "    conv_2 = b_conv * 2\n",
    "    conv_3 = b_conv * 4\n",
    "    l_conv = b_conv / 2\n",
    "    chan = 1\n",
    "    input_layer = tf.keras.layers.Input(shape = (ny, nx, frames, chan))\n",
    "    #First encoder step\n",
    "    l1 = tf.keras.layers.Conv3D(b_conv, **conv3D_args)(input_layer)\n",
    "    mp1 = tf.keras.layers.MaxPooling3D( **maxPool_args)(l1)\n",
    "    #Second encoder step\n",
    "    l2 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(mp1)\n",
    "    l3 = tf.keras.layers.Conv3D(conv_2, **conv3D_args)(l2)\n",
    "    mp2 = tf.keras.layers.MaxPooling3D( **maxPool_args)(l3)\n",
    "    #Third encoder step\n",
    "    lr = tf.keras.layers.Reshape((90, 160, conv_2))(mp2)\n",
    "    l4 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(lr)\n",
    "    l5 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(l4)\n",
    "    l6 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(l5)\n",
    "    l7 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(l6)\n",
    "    l8 = tf.keras.layers.Conv2D(conv_3, **conv2D_args)(l7)\n",
    "    #Decoding from third step\n",
    "    l9 = tf.keras.layers.UpSampling2D((2,2))(l8)\n",
    "    l10 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l9)\n",
    "    l11 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l10)\n",
    "    l12 = tf.keras.layers.Conv2D(conv_2, **conv2D_args)(l11)\n",
    "    #Decoding from third step\n",
    "    l13 = tf.keras.layers.UpSampling2D((2,2))(l12)\n",
    "    l14 = tf.keras.layers.Conv2D(b_conv, **conv2D_args)(l13)\n",
    "    l15 = tf.keras.layers.Conv2D(b_conv, **conv2D_args)(l14)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Conv2D(outChan, (3,3), padding = 'same', activation = lact)(l15)\n",
    "\n",
    "    #Autoencoder model\n",
    "    autoencoder = tf.keras.Model(input_layer, output_layer)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer = opt, loss = loss, metrics=['acc'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = getIntraSingleChModelMP()\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "#                                                      histogram_freq=1,\n",
    "#                                                      profile_batch = '500,520')\n",
    "h = autoencoder.fit(get_training_dataset(), \n",
    "                    validation_data = get_validation_dataset(),  \n",
    "                    epochs = epochs, \n",
    "                    verbose = 1) \n",
    "#                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trHist = np.array([h.history['acc'],\n",
    "                   h.history['val_acc'], \n",
    "                   h.history['loss'],\n",
    "                   h.history['val_loss']])\n",
    "\n",
    "epochs = range(len(trHist[0])) # Get list of numbers in length of epochs\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs[0:100], trHist[0], 'r', label = \"Training Accuracy\")\n",
    "plt.plot(epochs[0:100], trHist[1], 'b',label = \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs[0:100], trHist[2], 'r', label = \"Training Loss\")\n",
    "plt.plot(epochs[0:100], trHist[3], 'b', label = \"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../Datasets/DVRUP_1f_UINT8_82020/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset, reshape and predict some test images!\n",
    "def loadDataset(file, dir = \"./datasetDir/\"):\n",
    "    X = np.load(dir + file +\"DUINT8X.npy\")[:, :, :, :]\n",
    "    Y = np.load(dir + file +\"DUINT8Y.npy\")[:, :, :, :]\n",
    "    return X, Y\n",
    "\n",
    "vX, vY = loadDataset('Skogen7', dir = '../../Datasets/DVRUP_1f_UINT8_82020/')\n",
    "vX = vX.astype(float)/255.0\n",
    "vY = vY.astype(float)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the x dataset\n",
    "def getFramesAround(DS, num, fbf = 2, faf = 1):\n",
    "    \"\"\" Returns a matrix with the fbf frames before and the faf frames after frame number num \"\"\"\n",
    "    return np.stack(DS[(num-fbf):(1+num+faf)], axis = 3)\n",
    "\n",
    "def reshapeDStoFBF(X, Y, fbf = 2, faf = 1):\n",
    "    \"\"\" Reshapes the dataset so that each entry contains the fbf frames before and faf frames after the entry \"\"\"\n",
    "    rn = fbf\n",
    "    buf =  np.empty((len(X) - (faf + fbf), len(X[0]), len(X[0][0]),chan, frames), np.dtype('uint8'))\n",
    "    for i in range(len(X) - (faf + fbf)):\n",
    "        buf[rn - fbf] = getFramesAround(X, rn,fbf,faf)\n",
    "        rn += 1\n",
    "    print('Reshaped dataset to size: ', buf.shape)\n",
    "    return buf\n",
    "\n",
    "def normalizeDS(x):\n",
    "    x = x - x.mean()\n",
    "    x = x * x.std()\n",
    "    return x\n",
    "\n",
    "nX = reshapeDStoFBF(vX, vY, framesBefore, framesAfter)\n",
    "print('New shapes: x:', nX.shape, 'y', vY.shape)\n",
    "#Swapping the axes\n",
    "nX = np.swapaxes(nX, 3, 4)\n",
    "\n",
    "#nX = nX - nX.mean()\n",
    "#nX = nX * nX.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFrame(frame, title = 'Frame', show = True):\n",
    "    plt.imshow(frame)\n",
    "    plt.title(title)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting one frame and showing it\n",
    "f = 50\n",
    "valFrame = np.clip(autoencoder.predict(nX[f:f+1, :, :, :, 0], batch_size = batch_size), -1.0, 1.0)\n",
    "showFrame(vX[f, :, :, 0], title = 'Input')\n",
    "showFrame(valFrame[0, :, :], title = 'Output')\n",
    "#showFrame(valFrame[0, :, :, 0])\n",
    "#showFrame(valFrame[0, :, :, 1])\n",
    "#showFrame(valFrame[0, :, :, 2])\n",
    "#showFrame(valFrame[0, :, :, :])\n",
    "showFrame(vY[f, :, :, 0], title = 'Target')\n",
    "\n",
    "print('Experimenting with augmentation use')\n",
    "f2 = 100\n",
    "showFrame(vY[f2, :, :, 0], title = 'Target')\n",
    "showFrame(vY[f2, :, :, 0] + valFrame[0, : ,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processed: ')\n",
    "print('Max: ', valFrame[0].max(), 'Min:', valFrame[0].min(),' mean: ', valFrame[0].mean())\n",
    "print('Input: ')\n",
    "print('Max: ', vX[f, :, :, 0].max(), 'Min:', vX[f, :, :, 0].min(),' mean: ', vX[f, :, :, 0].mean())\n",
    "print('Output: ')\n",
    "print('Max: ', vY[f, :, :, 0].max(), 'Min:', vY[f, :, :, 0].min(),' mean: ', vY[f, :, :, 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outN = '81'\n",
    "lpath = '../TrainingResults/lossAndAcc/'\n",
    "spath = '../TrainingResults/stats/'\n",
    "np.save(lpath + outN + 'l', trHist)\n",
    "a = np.array([startTime, \n",
    "              initComment, \n",
    "              datasetSize, \n",
    "              datasetsToLoad, \n",
    "              framesSkipped,\n",
    "              (nx, ny), \n",
    "              framesBefore, \n",
    "              framesAfter, \n",
    "              baseConvLay, \n",
    "              usedLoss, \n",
    "              epochs,\n",
    "              trHist[3][-1],\n",
    "              trHist[1][-1],\n",
    "              'cyberspace - RTX3090',\n",
    "              ' ~ < 0.5h ',\n",
    "              ' Model might be useful for augmentation '], dtype = object)\n",
    "np.save(spath + outN +'c', a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modPath = '../Models/'\n",
    "autoencoder.save(modPath + outN +'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Experiment',  outN, 'executed using tensorflow V', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
